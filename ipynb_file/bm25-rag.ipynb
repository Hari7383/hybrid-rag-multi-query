{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98124cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\VS_Programs\\addvance_rag\\.ragvenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9140240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_PATH = \"datasetFED/\"\n",
    "PERSIST_DIR = \"./FEDcoma_db\"\n",
    "\n",
    "CHAT_URL = \"API\"\n",
    "LLM_MODEL = \"QuantTrio/Qwen3-VL-32B-Instruct-AWQ\"\n",
    "\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "TOP_K_DENSE = 6\n",
    "TOP_K_SPARSE = 6\n",
    "FINAL_TOP_K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb6d6493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x134c4f for key /Im7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 41333\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading documents...\")\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    DATASET_PATH,\n",
    "    glob=\"*.pdf\",\n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total chunks created: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6993b84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\VS_Programs\\addvance_rag\\.ragvenv\\lib\\site-packages\\huggingface_hub\\file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ads2509026\\AppData\\Local\\Temp\\1\\ipykernel_19912\\1216025742.py:14: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing Chroma DB.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=split_docs,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=PERSIST_DIR\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    print(\"Documents embedded and stored.\")\n",
    "else:\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=PERSIST_DIR,\n",
    "        embedding_function=embedding_model\n",
    "    )\n",
    "    print(\"Loaded existing Chroma DB.\")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K_DENSE})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38544929",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [doc.page_content for doc in split_docs]\n",
    "tokenized_corpus = [doc.split() for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad46636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\VS_Programs\\addvance_rag\\.ragvenv\\lib\\site-packages\\huggingface_hub\\file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51dfdb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query_multi(query):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Generate 3 alternative search queries for:\n",
    "\n",
    "\"{query}\"\n",
    "\n",
    "Make them more descriptive and include possible synonyms.\n",
    "Return each on a new line only.\n",
    "\"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": LLM_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You improve search queries for document retrieval.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    "\n",
    "    r = requests.post(CHAT_URL, json=payload, timeout=60)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    content = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    queries = [\n",
    "        line.strip(\"- \").strip()\n",
    "        for line in content.split(\"\\n\")\n",
    "        if line.strip()\n",
    "    ]\n",
    "\n",
    "    # Include original query\n",
    "    return list(set([query] + queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dddf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_retrieve_multi(query):\n",
    "\n",
    "    expanded_queries = expand_query_multi(query)\n",
    "\n",
    "    # print(\"\\n--- Expanded Queriesn ---\")\n",
    "    # print(expanded_queries)    \n",
    "\n",
    "    all_docs = []\n",
    "\n",
    "    for q in expanded_queries:\n",
    "\n",
    "        # Dense retrieval\n",
    "        dense_docs = retriever.invoke(q)\n",
    "\n",
    "        # Sparse retrieval\n",
    "        tokenized_query = q.split()\n",
    "        sparse_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "        top_sparse_idx = sorted(\n",
    "            range(len(sparse_scores)),\n",
    "            key=lambda i: sparse_scores[i],\n",
    "            reverse=True\n",
    "        )[:TOP_K_SPARSE]\n",
    "\n",
    "        sparse_docs = [split_docs[i] for i in top_sparse_idx]\n",
    "\n",
    "        all_docs.extend(dense_docs)\n",
    "        all_docs.extend(sparse_docs)\n",
    "\n",
    "    # Deduplicate\n",
    "    unique_docs = list({doc.page_content: doc for doc in all_docs}.values())\n",
    "\n",
    "    # Rerank\n",
    "    pairs = [(query, doc.page_content) for doc in unique_docs]\n",
    "    scores = reranker.predict(pairs)\n",
    "\n",
    "    ranked = sorted(\n",
    "        zip(scores, unique_docs),\n",
    "        reverse=True,\n",
    "        key=lambda x: x[0]\n",
    "    )\n",
    "\n",
    "    return [doc for _, doc in ranked[:FINAL_TOP_K]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a77a0c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm(docs, query):\n",
    "\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a document-based assistant.\n",
    "\n",
    "Rules:\n",
    "1. Answer ONLY using the provided context.\n",
    "2. If the topic is discussed indirectly, summarize what the document explains.\n",
    "3. If the topic is not mentioned at all, reply exactly:\n",
    "   \"Not found in the provided documents.\"\n",
    "4. Do NOT add external knowledge.\n",
    "5. Be clear and concise.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": LLM_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Answer strictly from document context.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    "\n",
    "    r = requests.post(CHAT_URL, json=payload, timeout=120)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    return r.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcabb0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG ready. Type 'exit' to quit.\n",
      "\n",
      "--- Question ---\n",
      "Updated Form 1099-K Reporting Thresholds\n",
      "\n",
      "LLM Answer:\n",
      "\n",
      "The updated Form 1099-K reporting thresholds require payment card companies, payment apps, and online marketplaces to send a Form 1099-K only if the amount of business transactions during the year meets or exceeds the reporting threshold. The document does not specify the exact dollar threshold, but it indicates that reporting is triggered based on the total amount of business transactions. Additionally, the form must be filed if the number of payment transactions (excluding refunds) processed through the payment card/third-party payer network meets certain criteria. The document also notes that the form and instructions are updated continuously and available at IRS.gov/Form1099K.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRAG ready. Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "\n",
    "    query = input(\"\\nAsk a question: \")\n",
    "\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    print(\"\\n--- Question ---\")\n",
    "    print(query)\n",
    "\n",
    "    docs = hybrid_retrieve_multi(query)\n",
    "\n",
    "    # print(\"\\n--- Docs ---\")\n",
    "    # print(docs)\n",
    "\n",
    "    answer = ask_llm(docs, query)\n",
    "\n",
    "    print(\"\\nLLM Answer:\\n\")\n",
    "    print(answer)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ragvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
